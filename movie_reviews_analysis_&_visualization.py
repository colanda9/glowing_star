# -*- coding: utf-8 -*-
"""Movie reviews analysis & visualization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11qcGYOGA2kJE5pnwSXlDwN5V7Syhf-Ad
"""

!pip install pandasql

!pip install pandas==1.1.5

import pandas as pd
import datetime as dt
import re
import pandasql as ps #SQL on Pandas Dataframe
import nltk
nltk.download('punkt')

from wordcloud import WordCloud
import matplotlib.pyplot as plt 
from collections import Counter
import random

# Three datasets we're using
! wget -nc https://storage.googleapis.com/penn-cis545/rotten_tomatoes_movies.csv
! wget -nc https://storage.googleapis.com/penn-cis545/rotten_tomatoes_critic_reviews.csv
! wget -nc https://storage.googleapis.com/penn-cis545/MoviesOnStreamingPlatforms.csv

print(pd.__version__)
# Make sure it's 1.1.5

"""## Part 1: Load & Process our Datasets

Before we get into the data, we first need to load and clean our datasets. 

* Load and save the `rotten_tomatoes_movies.csv` to a dataframe called `movies_df`.
* Load and save the `rotten_tomatoes_critic_reviews.csv` to a dataframe called `reviews_df`.
* Load and save the `MoviesOnStreamingPlatforms.csv` to a dataframe called `streaming_df` without the index column being included.
"""

# Import the datasets to pandas dataframes
movies_df = pd.read_csv("rotten_tomatoes_movies.csv")
reviews_df = pd.read_csv("rotten_tomatoes_critic_reviews.csv")
streaming_df =  pd.read_csv("MoviesOnStreamingPlatforms.csv")

# view movies_df to make sure the import was successful
movies_df.head(2)

# view reviews_df to make sure the import was successful
reviews_df.head(2)

# view streaming_df to make sure the import was successful
streaming_df.head(2)

"""### 1.1 Data Preprocessing

Next, we are going to want to clean up our dataframes by 1) fixing column names, 2) changing datatypes, 3) cleaning text, and 4) handling nulls.

First, let's do EDA
.
"""

# view info information regarding movies_df
movies_df.info()

# clean movies_df
movies_df = movies_df.drop(columns=['critics_consensus','production_company'])
movies_df['genres'] = movies_df['genres'].fillna("No Genre")
movies_df = movies_df.dropna(axis=0)
movies_df["original_release_date"] = pd.to_datetime(movies_df["original_release_date"],unit='ns')
movies_df['streaming_release_date'] = pd.to_datetime(movies_df['streaming_release_date'],unit='ns')
movies_df['runtime'] = movies_df['runtime'].astype('int64')
movies_df['tomatometer_count'] = movies_df['tomatometer_count'].astype('int64')
movies_df['tomatometer_rating'] = movies_df['tomatometer_rating'].astype('int64')
movies_df['audience_count'] = movies_df['audience_count'].astype('int64')
movies_df['audience_rating'] = movies_df['audience_rating'].astype('int64')

# create new dataframe and genre column
exploded_movies_df = movies_df
exploded_movies_df['genre']=exploded_movies_df['genres'].str.split(',')
exploded_movies_df = exploded_movies_df.explode('genre')
exploded_movies_df['genre']=exploded_movies_df['genre'].apply(lambda x: x.strip(" "))

#view info of reviews_df
reviews_df.info()

#Clean reviews_df
reviews_df = reviews_df.dropna(subset=['review_score','review_content'],axis = 0)
reviews_df['critic_name'] = reviews_df['critic_name'].fillna("Anonymous")
reviews_df['review_date']=pd.to_datetime(reviews_df['review_date'],unit='ns')

# more EDA here!
reviews_df.describe()

reviews_df['critic_name'].unique()

"""## Part 2: Exploring the Data with PandasSQL and Pandas

###2.1 Movie Recommendations

#### 2.1.1 What movies have good reviews from critics and audience?

`movies_df` contains all sorts of movies. We all love good movies, so let's try to separate the good from the bad.
"""

# Use pandas to obtain good_critics_df
good_critics_df = movies_df[(movies_df['tomatometer_status']=="Certified-Fresh") | (movies_df['tomatometer_status']=="Fresh")]
good_critics_df = good_critics_df[["rotten_tomatoes_link","movie_title"]]

"""then, let's consider the ratings by movie-goers.

"""

movies_df = movies_df.drop(columns=['genre'])

# Use pandasql to obtain movies with good audience rating
good_audience_query = """SELECT rotten_tomatoes_link, movie_title
FROM movies_df
WHERE audience_status = 'Upright'
"""

good_audience_df = ps.sqldf(good_audience_query,locals())

"""Now, we can simply join these dataframes to get a table of all the best movies and binge watch one every night! But wait! Sometimes, the opinions of critics and audience may differ, so we need to make sure that the good movies we watch are ones that are viewed positively by both critics and audiences. 

Thus, we use **pandas and pandasql** to filter out movies with **mixed reviews** (ie. audience and critic opinions differ) to a new dataframe `pd/sql_mixed_movies_df`, ordered by lexicographic order of `movie_title`.
"""

import numpy as np

# Filter out movies with mixed reviews using pandas
pd_mixed_movies_df = good_critics_df.merge(good_audience_df,how='outer',on=['rotten_tomatoes_link'],indicator=True)
pd_mixed_movies_df = pd_mixed_movies_df[pd_mixed_movies_df["_merge"] != "both"]
pd_mixed_movies_df = pd_mixed_movies_df.rename(columns={'movie_title_x': 'movie_title'})
pd_mixed_movies_df['movie_title'] = np.where(pd_mixed_movies_df["_merge"] == "right_only",pd_mixed_movies_df["movie_title_y"],pd_mixed_movies_df['movie_title'])
pd_mixed_movies_df = pd_mixed_movies_df.drop(['_merge','movie_title_y'],axis=1)
pd_mixed_movies_df = pd_mixed_movies_df.sort_values(by='movie_title')
pd_mixed_movies_df

# Filter out movies with mixed reviews from good_critics_df and good_audience_df
mixed_query = """
SELECT rotten_tomatoes_link, movie_title 
FROM 
  (
    SELECT rotten_tomatoes_link, movie_title FROM good_critics_df
    EXCEPT SELECT rotten_tomatoes_link, movie_title FROM good_audience_df
  ) 
UNION 
SELECT rotten_tomatoes_link, movie_title 
FROM 
  (
    SELECT rotten_tomatoes_link, movie_title FROM good_audience_df
    EXCEPT SELECT rotten_tomatoes_link, movie_title FROM good_critics_df
  ) 
ORDER BY movie_title ASC
"""
#https://www.red-gate.com/simple-talk/databases/sql-server/performance-sql-server/the-except-and-intersect-operators-in-sql-server/
sql_mixed_movies_df = ps.sqldf(mixed_query,locals())
sql_mixed_movies_df

"""**TODO:**

Using **pandas/pandasql** and `pd/sql_mixed_movies_df`, find the top 10 mixed movies with the **largest absolute difference** in critic review score and audience review score. You should calculate this as the absolute difference between `tomatometer_rating` and `audience_rating`, and store this in a column named `diff`. If multiple movies have the same `diff` value, order their rows in lexicographic order.

Format the output as a dataframe called `pd/sql_top_10_mixed_movies_df` that has the following schema:

>movie_title | diff
>--- | ---

"""

# TODO: pandas
df_diff = pd_mixed_movies_df.merge(movies_df,how='left',on=['rotten_tomatoes_link','movie_title'])
df_diff['diff'] = np.abs(df_diff['tomatometer_rating'] - df_diff['audience_rating'])
df_diff = df_diff.sort_values(by=['diff','movie_title'],ascending=[False,True])
pd_top_10_mixed_movies_df = df_diff[['movie_title','diff']].head(10)

#TODO: pandasql
top_10_mixed_query = """
SELECT movie_title, Abs(tomatometer_rating - audience_rating) AS diff
FROM(
SELECT sql_mixed_movies_df.movie_title, movies_df.tomatometer_rating, audience_rating
FROM sql_mixed_movies_df
LEFT JOIN movies_df
ON sql_mixed_movies_df.rotten_tomatoes_link = movies_df.rotten_tomatoes_link)
ORDER BY diff DESC, movie_title ASC
LIMIT 10
"""

sql_top_10_mixed_movies_df = ps.sqldf(top_10_mixed_query,locals())
#https://www.tutorialspoint.com/sorted-difference-between-two-columns-in-mysql

"""#### 2.1.2 Horror Movie recommendation

Are there more horror movies during October (because of Halloween)? 

"""

# pandas version
movies_df['month']= movies_df['original_release_date'].apply(lambda x: x.month)
df_horror = movies_df[movies_df['genres'].str.contains('Horror')]
pd_horror_df = df_horror.groupby('month',as_index=False).count()
#pd_horror_df = pd.DataFrame({'num_movies' : movies_df.groupby('month',as_index=False).count()}).reset_index()
pd_horror_df = pd_horror_df[['month','rotten_tomatoes_link']]
pd_horror_df = pd_horror_df.rename(columns={'rotten_tomatoes_link':'num_movies'})
pd_horror_df = pd_horror_df.sort_values(by=['num_movies'],ascending=False)
pd_horror_df

# pandasql version
horror_query = """
SELECT month, COUNT(rotten_tomatoes_link) AS num_movies
FROM(
  SELECT *
FROM movies_df M
WHERE M.genres LIKE '%Horror%')
GROUP BY month
ORDER BY num_movies DESC
"""

sql_horror_df = ps.sqldf(horror_query,locals())
sql_horror_df

"""It seems like October is not the most popular month in which horror movies are released. 

Then, let's find the movies classified as `Horror` that were originally released in **January**.
"""

# pandas version
df_horror = movies_df[movies_df['genres'].str.contains('Horror')]
df_horror_jan = df_horror[df_horror['month']==1]
pd_jan_df = df_horror_jan[['rotten_tomatoes_link','movie_title','audience_count']]
pd_jan_df = pd_jan_df.sort_values(by='audience_count',ascending=False)

# pandasql version
jan_query = """
SELECT rotten_tomatoes_link, movie_title, audience_count
FROM(
  SELECT *
FROM movies_df M
WHERE M.genres LIKE '%Horror%')
WHERE month = 1
ORDER BY audience_count DESC
"""

sql_jan_df = ps.sqldf(jan_query,locals())
sql_jan_df

"""With all the information about these movies, we are ready to make movie recommendations. In particular, let's find a movie that: 
- is a horror movie
- was originally released in January 
- has been watched by at least 50000 audience members, with an overall `audience_rating` of `Upright`.  
"""

# use pandasql to find top 10 movies that fit criteria
recs_query = """
SELECT rotten_tomatoes_link, movie_title, audience_count
FROM
(SELECT *
 FROM pd_jan_df P INNER JOIN good_audience_df G
 ON P.rotten_tomatoes_link = G.rotten_tomatoes_link)
ORDER BY audience_count DESC
LIMIT 10
"""

movie_recs_df = ps.sqldf(recs_query,locals())
movie_recs_df

"""### 2.2 Good Critic Reviews 

Now let us switch gears and find the critics who are good at writing reviews.

The first criteria that defines a good reviewer is balance and objectivity. Just like people (other than me), no movie is perfect, and so we probably want reviews to cover both the good and the bad. 

Therefore, by using `reviews_df` and **pandasql**, we can find the reviews that contain both the word `"good"` and the word `"bad"` (case-insensitive). Include substrings (ie. it can be part of a word, such as '**good**-natured' or 'for**bad**e').

"""

# use pandasql to find reviews with both words "good" and "bad"
balanced_query = """
SELECT critic_name, review_content
FROM reviews_df R
WHERE (LOWER(R.review_content) LIKE '%good%' 
AND LOWER(R.review_content) LIKE '%bad%')
ORDER BY R.review_content ASC
"""
balanced_df = ps.sqldf(balanced_query,locals())
balanced_df.head()
#https://www.w3schools.com/sql/sql_wildcards.asp

"""*Good cause, bad movie.* One of my favourite quotes.

The second criteria we have for good critics is tenure. Let us find the "seasoned" reviewers, ie. those who have been writing reviews for a long period of time.

By using `reviews_df` and **pandasql**, we
- Create a column `date_diff` which contains the difference (in number of days) between each critic's first review and most recent review
- Only include reviews from the 21st Century (ie. published from 2000 onward)
- Only include critics whose first and most recent reviews are at least 10 years **(3652 days)** apart
- Do not include reviews by `'Anonymous'`

"""

# use pandasql to find seasoned reviewers
time_query = """
SELECT critic_name, CAST(julianday(MAX(review_date))-julianday(MIN(review_date)) AS Integer) AS date_diff
FROM  reviews_df
WHERE (critic_name != 'Anonymous' AND strftime('%Y', review_date) >= '2000')
GROUP BY critic_name
HAVING date_diff >= 3652
ORDER BY date_diff DESC
"""

critics_time_df = ps.sqldf(time_query,locals())
critics_time_df.head(5)
#https://www.sqlite.org/lang_datefunc.html

"""### 2.3 What do we watch on Netflix tonight?

Now let's run a tougher analysis. We've all experienced the aimless scrolling through the Netflix homepage, trying (and sometimes failing) to find a movie that interests us. It's painful to manually search up the reviews of each movie we see, so why don't we try to automate this process? Let's determine the best on Netflix for each genre using both critics' and audiences' opinions on Rotten Tomatoes.
"""

# use pandasql to find the best on Netflix
netflix_best_query = """
WITH TB AS
(
SELECT genre_category, combined_rt_rating 
FROM (
SELECT Title AS movie_title, genres, genre AS genre_category, max(combined_rt_rating) AS combined_rt_rating 
FROM(
  SELECT s.Title, e.genres, e.genre, e.tomatometer_rating + e.audience_rating AS combined_rt_rating
FROM streaming_df s LEFT JOIN exploded_movies_df e 
ON s.Title = e.movie_title
WHERE s.Netflix = 1)
GROUP BY genre_category
))

SELECT e.movie_title, e.genres, e.genre AS genre_category, e.tomatometer_rating + e.audience_rating AS combined_rt_rating 
FROM exploded_movies_df e JOIN streaming_df s
ON s.Title = e.movie_title
WHERE s.Netflix = 1 AND (genre_category, combined_rt_rating) IN TB
ORDER BY genre_category
"""

netflix_best_df = ps.sqldf(netflix_best_query,locals())
netflix_best_df

"""### 2.4 Which months of the year does each streaming service have the most number of movies available?

Maybe you don't care about picking out the "good" movies from the "bad" ones. Maybe you just want to watch as many movies as possible each month. Still, paying those monthly bills for all these streaming services (which seem to be increasing in number every day) doesn't seem to be a sustainable strategy. It would be much easier to pay for certain streaming services in the month they have the most movies.

Thus, by using `movies_df` and `streaming_df`, we can
- Count the number of movies for each streaming service per month using the `month` column in `movies_df` you created earlier.
- For each streaming service, find the month in which they have the most movies available and how many movies are available in that month. Include ties, if there are any.
"""

#  use pandasql to find the month where each streaming service has the most movies
best_streaming_query = """
WITH titles AS (
  SELECT * 
  FROM movies_df m INNER JOIN streaming_df s ON m.movie_title = s.Title
  ),

  net AS( SELECT 'Netflix' as streaming_service, a.month, count(*) AS count 
  FROM titles a 
  WHERE a.Netflix = 1 
  GROUP BY a.month
  ),

  hulu AS(SELECT 'Hulu' as streaming_service, a.month, count(*) AS count 
  FROM titles a 
  WHERE a.Hulu = 1 
  GROUP BY a.month
  ),

  prime AS(SELECT 'Prime Video' as streaming_service, a.month, count(*) AS count 
  FROM titles a 
  WHERE a.[Prime Video] = 1 
  GROUP BY a.month
  ),

  disney AS(SELECT 'Disney+' as streaming_service, a.month, count(*) AS count 
  FROM titles a 
  WHERE a.[Disney+] = 1 
  GROUP BY a.month
  )

SELECT streaming_service, month AS max_month, max(count) AS num_movies 
FROM (
  SELECT * FROM net
  UNION ALL 
  SELECT * FROM hulu
  UNION ALL 
  SELECT * FROM prime
  UNION ALL 
  SELECT * FROM disney
)
GROUP BY streaming_service
"""

best_streaming_df = ps.sqldf(best_streaming_query,locals())
best_streaming_df

"""### 2.5 What movies are underrated by critics (ft. top critics)?

[According to the Rotten Tomatoes website](https://www.rottentomatoes.com/critics/top_critics), "Top Critic is a designation created to distinguish Tomatometer-approved critics who excel at their craft. Critics selected are well-established, influential, and prolific..." Although all critics on RT are officially approved, we want to establish a top-critic-only score that only includes the opinions of the "cream of the crop". Certain films could be poorly received by regular critics, but top critics see something special in them. Let's try and find these underappreciated movies!
"""

# use pandasql to find the regular critics' underrated movies
underrated_movies_query = """
WITH fresh_tb AS (
  SELECT m.rotten_tomatoes_link, COUNT(*) AS f_count 
  FROM reviews_df r JOIN movies_df m ON r.rotten_tomatoes_link = m.rotten_tomatoes_link
  WHERE (r.top_critic = 1 AND r.review_type = 'Fresh')
  GROUP BY m.rotten_tomatoes_link
), 
total_tb AS (
  SELECT m.rotten_tomatoes_link, COUNT(*) AS t_count 
  FROM reviews_df r JOIN movies_df m ON r.rotten_tomatoes_link = m.rotten_tomatoes_link
  WHERE (r.top_critic = 1)
  GROUP BY m.rotten_tomatoes_link 
  HAVING COUNT(*) > 5
), 
tct AS (
  SELECT f.rotten_tomatoes_link, CAST(CAST(f.f_count AS float) / CAST(t.t_count AS float) * 100 AS Int) AS top_critic_tomatometer 
  FROM fresh_tb f JOIN total_tb t ON f.rotten_tomatoes_link = t.rotten_tomatoes_link
)

SELECT m.movie_title, tct.top_critic_tomatometer, m.tomatometer_rating, (top_critic_tomatometer - tomatometer_rating) AS score_diff 
FROM tct JOIN movies_df m ON tct.rotten_tomatoes_link = m.rotten_tomatoes_link
ORDER BY score_diff DESC 
LIMIT 10
"""

underrated_movies_df = ps.sqldf(underrated_movies_query,locals())
underrated_movies_df

"""## Part 3: Working with Text Data

Shifting gears, let's now try to do some text-based analysis. Text data is complex, but can also be used to generate extremely interpretable results, making it valuable and interesting. 

**How do reviews by top critics differ from reviews made by non-top critics?**

###3.1 Extract Data
"""

# create two dataframes & two lists for top and regular critics
top_critics_df = reviews_df[reviews_df['top_critic']==True]
regular_critics_df = reviews_df[reviews_df['top_critic']==False]

top_content = list(top_critics_df['review_content'])
regular_content =list(regular_critics_df['review_content'])

"""###3.2 Tokenize the Text

Here, we are going to split up the content into a list of words. We will use the **nltk** package, which contains an extensive set of tools to process text. 

First, we use **stopwords** to create a set of the most common english stopwords. Then, we implement **tokenized_content(content)** that takes in a content string and:
1. tokenizes the text
2. lowercases the token
3. removes stop words (commonly used words such as "a","an", "in")
4. keeps words with only alphabet characters (no punctuation)
"""

import nltk
nltk.__version__

from nltk.corpus import stopwords
nltk.download('stopwords')
stopwords = set(stopwords.words('english'))
# Note that stopwords are all in lowercase format

# Create tokenized_content(content) function
def tokenized_content(content):
  new_content = []
  for mytext in content:
    mytext = nltk.word_tokenize(mytext)
    for word in mytext:
      word = word.lower() 
      if word.isalpha():
        if not word in stopwords:
          new_content.append(word)
  return new_content

"""Now, apply  `tokenize_content()` function to each piece of content in **top_content** and **regular_content** and flatten both of the lists to create **top_tokens** and **regular_tokens**

"""

# tokenize and flatten
top_tokens = tokenized_content(top_content)
#top_tokens = [item for sublist in top_tokens for item in sublist]

regular_tokens = tokenized_content(regular_content)

"""### 3.3 Most Frequent Words
Now, let's find the 20 most common words amongst the content of `top_tokens` and `regular_tokens`. Return this as a list of `(word, count)` tuples, in descending order of `count`.
"""

# Find 20 most common words amongst the content of the top and regular critic reviews
top_most_common = Counter(top_tokens).most_common(20)
regular_most_common = Counter(regular_tokens).most_common(20)

print(top_most_common)
print(regular_most_common)

"""###3.4 Refining our Lists

Hmmm...both of these lists seem to display similar words. Let's try to tease out words that distinguish the high from the low scoring questions. 

One approach would be to find words in one list that are not in the other. This, however, may be too naive, as even if a word is extremely common in our high list, if it appears only once in our low list, it would get removed from consideration.

Let's instead find the difference between the counts within our two lists. Thus, if a word is really common in one, but not the other, the count would only decrease slightly. Alternatively, if a word is common in both lists, it would effectively zero out. 

However, given that the number of regular tokens is about three times that of top tokens, we need to make sure both lists are of equal length before applying the difference method. 
"""

# randomly sample 1 million tokens from top_tokens and regular_tokens
import random
top_sample = Counter(random.sample(top_tokens,1000000))
regular_sample = Counter(random.sample(regular_tokens,1000000))

# use difference to find the top 20 counts of words within each group
distinct_top_most_common = (top_sample - regular_sample).most_common(20)
distinct_regular_most_common =(regular_sample - top_sample).most_common(20)

print(distinct_top_most_common)
print(distinct_regular_most_common)

"""### 3.5 Word Clouds

Before we move on from this dataset, let's do one final step and visualize our results with wordclouds.
"""

# make a word cloud for top tokens
top_counter = top_sample - regular_sample
wordcloud = WordCloud(background_color = 'white').generate_from_frequencies(top_counter)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()
#https://www.datacamp.com/community/tutorials/wordcloud-python

# make a word cloud for regular tokens
regular_counter = regular_sample - top_sample
wordcloud = WordCloud(background_color = 'white').generate_from_frequencies(regular_counter)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()